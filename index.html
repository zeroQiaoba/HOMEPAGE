<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>

  <title>Zheng Lian</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="description" content="Zheng Lian is currently a assistant researcher at National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA)">
  <meta name="keywords" content="Zheng Lian, 连政, Deep Learning, Affective Computing">
  <meta name="author" content="Zheng Lian" />
  <link rel="stylesheet" href="w3.css">

  <style>
  .w3-sidebar a {font-family: "Roboto", sans-serif}
  body,h1,h2,h3,h4,h5,h6,.w3-wide {font-family: "Montserrat", sans-serif;}
  </style>

  <link rel="icon" type="image/png" href="images/icons.png">
  <!--
  <script src="jquery.min.js"></script>
  <script>
  $(document).ready(function(){
    // Add smooth scrolling to all links
    $("a").on('click', function(event) {

      // Make sure this.hash has a value before overriding default behavior
      if (this.hash !== "") {
        // Prevent default anchor click behavior
        event.preventDefault();

        // Store hash
        var hash = this.hash;

        // Using jQuery's animate() method to add smooth page scroll
        // The optional number (800) specifies the number of milliseconds it takes to scroll to the specified area
        $('html, body').animate({
          scrollTop: $(hash).offset().top
        }, 800, function(){

          // Add hash (#) to URL when done scrolling (default click behavior)
          window.location.hash = hash;
        });
      } // End if
    });
  });
  </script>
  //-->

</head>


<body class="w3-content" style="max-width:1000px">

<!-- Sidebar/menu -->
<nav class="w3-sidebar w3-bar-block w3-black w3-collapse w3-top w3-right" style="z-index:3;width:150px" id="mySidebar">
  <div class="w3-container w3-display-container w3-padding-16">
    <h3><b>Zheng Lian</b></h3>
  </div>
  <div class="w3-padding-64 w3-text-light-grey w3-large" style="font-weight:bold">
    <a href="#home" class="w3-bar-item w3-button">Home</a>
    <a href="#publications" class="w3-bar-item w3-button">Research</a>
    <a href="#award" class="w3-bar-item w3-button">Awards</a>
	<a href="#professionals" class="w3-bar-item w3-button">Academic Services</a>
	<a href="#projects" class="w3-bar-item w3-button">Projects</a>
	<a href="#patents" class="w3-bar-item w3-button">Patents</a>
  </div>
</nav>

<!-- Top menu on small screens -->
<header class="w3-bar w3-top w3-hide-large w3-black w3-xlarge">
  <div class="w3-bar-item w3-padding-24">Zheng Lian</div>
  <a href="javascript:void(0)" class="w3-bar-item w3-button w3-padding-24 w3-right"  style="font-stretch: extra-expanded;" onclick="w3_open()"><b>≡</b></a>
  </div>
</header>

<!-- Overlay effect when opening sidebar on small screens -->
<div class="w3-overlay w3-hide-large" onclick="w3_close()" style="cursor:pointer" title="close side menu" id="myOverlay"></div>

<!-- !PAGE CONTENT! -->
<div class="w3-main" style="margin-left:150px">

  <!-- Push down content on small screens -->
  <div class="w3-hide-large" style="margin-top:83px"></div>

<!-- The Home Section -->
    <div class="w3-container w3-center w3-padding-32" id="home">
      <img style="width: 80%;max-width: 200px" alt="profile photo" src="images/ZhengLian.png">
      <h1>Zheng Lian</h1>
        <p class="w3-justify" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;max-width:600px">
          I am an Assistant Researcher at Institute of Automation, Chinese Academy of Sciences (CASIA).
		  I received the Ph.D degree from CASIA in 2021.
		  My research primarily centers on affective computing, multimodal learning and noisy label learning. 
		  <strong>Recently, we aim to establish a new pathway toward more reliable affective computing techniques. See <a style="color: #447ec9" href="https://github.com/zeroQiaoba/AffectGPT">AffectGPT </a> for more details.</strong>
		</p>
        <p class="w3-center">
          <a href="lianzheng2016@ia.ac.cn">Email</a> &nbsp/&nbsp
          <a href="https://scholar.google.com/citations?user=S34nWz0AAAAJ&hl=en&oi=ao">Google Scholar</a>
        </p>
        </tbody></table>
  </div>


 <!-- The Publications Section -->
  <div class="w3-container w3-padding-32"" id="publications">
    <h2> Research </h2>
    <ol>
	  
	  <p>
      <li><strong>Explainable Multimodal Emotion Reasoning (AffectGPT)</strong>
      <br>
	  <strong>Zheng Lian</strong>, Licai Sun, Mingyu Xu, Haiyang Sun, Ke Xu, Zhuofan Wen, Shun Chen, Bin Liu, Jianhua Tao
      <br>
	  <em> Arxiv </em> 2023 | <a style="color: #447ec9" href="https://arxiv.org/pdf/2306.15401.pdf">paper</a>
      </p>
	  
	  <p>
      <li><strong>GCNet: Graph Completion Network for Incomplete Multimodal Learning in Conversation</strong>
      <br>
	  <strong>Zheng Lian</strong>, Lan Chen, Licai Sun, Bin Liu, Jianhua Tao
      <br>
      <em> Transactions on Pattern Analysis and Machine Intelligence (TPAMI) </em> 2023 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10008078">Accept</a>
      </p>
	  
	  <p>
      <li><strong>MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning</strong>
      <br>
	  <strong>Zheng Lian</strong>, Haiyang Sun, Licai Sun, Kang Chen, Mingyu Xu, Kexin Wang, etc.
      <br>
      <em> ACM Multimedia (Grand Challenge) </em> 2023 |  <a style="color: #447ec9" href="https://arxiv.org/pdf/2307.02227.pdf">paper</a>
      </p>
	
	  <p>
      <li><strong>ALIM: Adjusting Label Importance Mechanism for Noisy Partial Label Learning</strong>
      <br>
	  Mingyu Xu*, <strong>Zheng Lian*</strong>, Lei Feng, Bin Liu, Jianhua Tao
      <br>
      <em> NeurIPS </em> 2023 [Equal Contribution] |  <a style="color: #447ec9" href="https://arxiv.org/pdf/2301.12077.pdf">paper</a>
      </p>
	  
	  <p>
      <li><strong>VRA: Variational Rectified Activation for Out-of-distribution Detection</strong>
      <br>
	  Mingyu Xu, <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> NeurIPS </em> 2023 |  <a style="color: #447ec9" href="https://arxiv.org/pdf/2302.11716.pdf">paper</a>
      </p>
	  
	  
	  <p>
      <li><strong>MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic Facial Expression Recognition</strong>
      <br>
	  Licai Sun, <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> ACM Multimedia </em> 2023 |  <a style="color: #447ec9" href="https://arxiv.org/pdf/2307.02227.pdf">paper</a>
      </p>
	  
	  <p>
      <li><strong>Efficient multimodal transformer with dual-level feature restoration for robust multimodal sentiment analysis</strong>
      <br>
	  Licai Sun, <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> IEEE Transactions on Affective Computing (TAC) </em> 2023 |  <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/10122560">Early Access</a>
      </p>

	  <p>
      <li><strong>EmotionNAS: Two-stream Architecture Search for Speech Emotion Recognition</strong>
      <br>
      Haiyang Sun*, <strong>Zheng Lian*</strong>, Bin Liu, Ying Li, Licai Sun, Cong Cai, Jianhua Tao, Meng Wang, Yuan Cheng
      <br>
      <em>Interspeech </em> 2023 [Equal Contribution]| <a style="color: #447ec9" href="https://arxiv.org/abs/2203.13617">paper</a>
      </p>
	  
	  <p>
      <li><strong>PIRNet: Personality-enhanced Iterative Refinement Network for Emotion Recognition in Conversation</strong>
      <br>
	  <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> IEEE Transactions on Neural Networks and Learning Systems (TNNLS) </em> 2022 |  <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9839579">Early Access</a>
      </p>
	  
	  <p>
      <li><strong>SMIN: Semi-supervised Multi-modal Interaction Network for Conversational Emotion Recognition</strong>
      <br>
	  <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em> IEEE Transactions on Affective Computing (TAC) </em> 2022 |  <a style="color: #447ec9" href="https://ieeexplore.ieee.org/document/9674867">Early Access</a>
      </p>
		
	  <p>
      <li><strong>Emotional Reaction Analysis based on Multi-Label Graph Convolutional Networks and Dynamic Facial Expression Recognition Transformer</strong>
      <br>
	  Kexin Wang, <strong>Zheng Lian</strong>, Licai Sun, Bin Liu, Jianhua Tao, Yin Fan
	  <br>
      <em> Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge </em> 2022 | <a style="color: #447ec9" href="https://dl.acm.org/doi/abs/10.1145/3551876.3554810">paper</a>
      </p>
	  
	  <p>
      <li><strong>Multimodal Temporal Attention in Sentiment Analysis</strong>
      <br>
	  Yu He, Licai Sun, <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao, Meng Wang, Yuan Cheng
	  <br>
      <em> Proceedings of the 3rd International on Multimodal Sentiment Analysis Workshop and Challenge </em> 2022 [Winner] | <a style="color: #447ec9" href="https://dl.acm.org/doi/abs/10.1145/3551876.3554811">paper</a>
      </p>
	  
	  <p>
      <li><strong>AMOA: Global Acoustic Feature Enhanced Modal-Order-Aware Network for Multimodal Sentiment Analysis</strong>
      <br>
	  Ziming Li, Yan Zhou, Weibo Zhang, Yaxin Liu, Chuanpeng Yang, <strong>Zheng Lian</strong>, Songlin Hu
	  <br>
      <em> Proceedings of the 29th International Conference on Computational Linguistics (COLING) </em> 2022 | <a style="color: #447ec9" href="https://aclanthology.org/2022.coling-1.623.pdf">paper</a>
      </p>
	  
	  <p>
      <li><strong>Investigation of Multimodal Features, Classifiers and Fusion Methods for Emotion Recognition</strong>
      <br>
	  <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao
      <br>
      <em>
	  <em> National Conference on Man-Machine Speech Communication (NCMMSC) </em> 2021 [Best paper] | <a style="color: #447ec9" href="https://arxiv.org/abs/1809.06225">paper</a>
      </p>
	  
	  <p>
      <li><strong>DECN: Dialogical Emotion Correction Network for Conversational Emotion Recognition</strong>
      <br>
      <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao 
      <br>
      <em>Neurocomputing</em> 2021 | <a style="color: #447ec9" href="https://www.sciencedirect.com/science/article/pii/S0925231221007542">paper</a>
      </p>
	  
	  <p>
      <li><strong>CTNet: Conversational Transformer Network for Emotion Recognition</strong>
      <br>
      <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao 
      <br>
      <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)</em> 2021 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9316758">paper</a>
      </p>
	  
	  <p>
      <li><strong>Towards Fine-Grained Prosody Control for Voice Conversion</strong>
      <br>
      <strong>Zheng Lian</strong>, Rongxiu Zhong, Zhengqi Wen, Bin Liu, Jianhua Tao
      <br>
      <em>Proceedings of the 12th International Symposium on Chinese Spoken Language Processing (ISCSLP) </em> 2021 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9362110">paper</a>
      </p>
	  
	  <p>
      <li><strong>Multimodal Emotion Recognition and Sentiment Analysis via Attention Enhanced Recurrent Model</strong>
      <br>
	  Licai Sun, Mingyu Xu, <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao, Meng Wang, and Yuan Cheng
      <br>
      <em> Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge </em> 2021 [Winner] | <a style="color: #447ec9" href="https://dl.acm.org/doi/abs/10.1145/3475957.3484456">paper</a>
      </p>
	  
	  <p>
      <li><strong>Multimodal Sentiment Analysis based on Recurrent Neural Network and Multimodal Attention</strong>
      <br>
	  Cai, Cong, Yu He, Licai Sun, <strong>Zheng Lian</strong>, Bin Liu, Jianhua Tao, Mingyu Xu, and Kexin Wang
      <br>
      <em> Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge </em> 2021 [Winner] | <a style="color: #447ec9" href="https://dl.acm.org/doi/abs/10.1145/3475957.3484454">paper</a>
      </p>
	  
	  <p>
      <li><strong>Multimodal Cross-and Self-Attention Network for Speech Emotion Recognition</strong>
      <br>
	  Licai Sun, Bin Liu, Jianhua Tao, <strong>Zheng Lian</strong>
      <br>
      <em> IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em> 2021 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9414654">paper</a>
      </p>
	  
	  <p>
      <li><strong>An Improved Multimodal Dimension Emotion Recognition Based on Different Fusion Methods</strong>
      <br>
	  Haiyang Su, Bin Liu, Jianhua Tao, Yongfeng Dong, Jian Huang, <strong>Zheng Lian</strong>, Leichao Song
      <br>
      <em> IEEE International Conference on Signal Processing (ICSP)</em> 2021 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9321008">paper</a>
      </p>
	  
	  <p>
      <li><strong>Conversational Emotion Recognition Using Self-Attention Mechanisms and Graph Neural Networks</strong>
      <br>
      <strong>Zheng Lian</strong>, Jianhua Tao, Bin Liu, Jian Huang, Zhanlei Yang, Rongjun Li
      <br>
      <em>Interspeech </em> 2020 | <a style="color: #447ec9" href="http://www.interspeech2020.org/uploadfile/pdf/Wed-1-9-6.pdf">paper</a>
      </p>
	  
	  <p>
      <li><strong>Context-Dependent Domain Adversarial Neural Network for Multimodal Emotion Recognition</strong>
      <br>
      <strong>Zheng Lian</strong>, Jianhua Tao, Bin Liu, Jian Huang, Zhanlei Yang, Rongjun Li
      <br>
      <em>Interspeech </em> 2020 | <a style="color: #447ec9" href="http://www.interspeech2020.org/uploadfile/pdf/Mon-1-9-9.pdf">paper</a>
      </p>
	
	  <p>
      <li><strong>Expression Analysis based on Face Regions in Real-world Conditions</strong>
      <br>
      <strong>Zheng Lian</strong>, Ya Li, Jianhua Tao, Jian Huang, Mingyue Niu
      <br>
      <em>International Journal of Automation and Computing (IJAC) </em> 2020 | <a style="color: #447ec9" href="http://www.ijac.net/cn/article/doi/10.1007/s11633-019-1176-9?utm_source=TrendMD&utm_medium=cpc&utm_campaign=International_Journal_of_Automation_and_Computing_TrendMD_0">paper</a>
      </p>
	  
	  <p>
      <li><strong>CASIA Voice Conversion System for the Voice Conversion Challenge 2020</strong>
      <br>
      <strong>Zheng Lian</strong>, Jianhua Tao, Zhengqi Wen, Rongxiu Zhong
      <br>
      <em>Proceedings of the Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge </em> 2020 | <a style="color: #447ec9" href="https://isca-speech.org/archive/VCC_BC_2020/pdfs/VCC2020_paper_4.pdf">paper</a>
      </p>
	  
	  <p>
      <li><strong>ARVC: An Auto-Regressive Voice Conversion System Without Parallel Training Data</strong>
      <br>
      <strong>Zheng Lian</strong>, Zhengqi Wen, Xinyong Zhou, Songbai Pu, Shengkai Zhang, Jianhua Tao
      <br>
      <em>Proceedings of the 21st Annual Conference of the International Speech Communication Association (Interspeech) </em> 2020 | <a style="color: #447ec9" href="http://www.interspeech2020.org/uploadfile/pdf/Thu-3-4-7.pdf">paper</a>
      </p>	  
	  
	  <p>
      <li><strong>Multi-modal Continuous Dimensional Emotion Recognition Using Recurrent Neural Network and Self-Attention Mechanism</strong>
      <br>
	  Licai Sun*, <strong>Zheng Lian*</strong>, Jianhua Tao, Bin Liu, Mingyue Niu
      <br>
      <em>Proceedings of the 1st International on Multimodal Sentiment Analysis in Real-life Media Challenge and Workshop </em> 2020 [Winner] | <a style="color: #447ec9" href="https://dl.acm.org/doi/abs/10.1145/3423327.3423672">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Learning Utterance-level Representations with Label Smoothing for Speech Emotion Recognition</strong>
      <br>
	  Jian Huang, Jianhua Tao, Bin Liu, <strong>Zheng Lian</strong>
      <br>
      <em>Proceedings of the 21st Annual Conference of the International Speech Communication Association(Interspeech) </em> 2020 | <a style="color: #447ec9" href="http://www.interspeech2020.org/uploadfile/pdf/Thu-2-2-1.pdf">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Multimodal Transformer Fusion for Continuous Emotion Recognition</strong>
      <br>
	  Jian Huang, Jianhua Tao, Bin Liu, <strong>Zheng Lian</strong>, Mingyue Niu
      <br>
      <em>Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) </em> 2020 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9053762">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Multimodal Spatiotemporal Representation for Automatic Depression Level Detection</strong>
      <br>
	  Mingyue Niu, Jianhua Tao, Bin Liu, Jian Huang, <strong>Zheng Lian</strong>
      <br>
      <em>IEEE Transactions on Affective Computing (TAC) </em> 2020 | <a style="color: #447ec9" href="https://ieeexplore.ieee.org/abstract/document/9226102">paper</a>
      </p>	

	  <p>
      <li><strong>Unsupervised Representation Learning with Future Observation Prediction for Speech Emotion Recognition</strong>
      <br>
      <strong>Zheng Lian</strong>, Jianhua Tao, Bin Liu, Jian Huang
      <br>
      <em>Proceedings of the 20st Annual Conference of the International Speech Communication Association (Interspeech) </em> 2019 | <a style="color: #447ec9" href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1582.pdf">paper</a>
      </p>	  

	  <p>
      <li><strong>Conversational Emotion Analysis via Attention Mechanisms</strong>
      <br>
      <strong>Zheng Lian</strong>, Jianhua Tao, Bin Liu, Jian Huang
      <br>
      <em>Proceedings of the 20st Annual Conference of the International Speech Communication Association (Interspeech) </em> 2019 | <a style="color: #447ec9" href="https://www.isca-speech.org/archive/Interspeech_2019/pdfs/1577.pdf">paper</a>
      </p>	 
	  
	  <p>
      <li><strong>Semi-supervised Ladder Networks for Speech Emotion Recognition</strong>
      <br>
	  Jianhua Tao, Jian Huang, Ya Li, <strong>Zheng Lian</strong>, Mingyue Niu
      <br>
      <em>International Journal of Automation and Computing (IJAC) </em> 2019 | <a style="color: #447ec9" href="https://link.springer.com/article/10.1007/s11633-019-1175-x">paper</a>
      </p>	 
	  
	  <p>
      <li><strong>Discriminative video representation with temporal order for micro-expression recognition</strong>
      <br>
	  Mingyue Niu, Jianhua Tao, Ya Li, Jian Huang, <strong>Zheng Lian</strong>
      <br>
      <em>Proceedings of the 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) </em> 2019 | <a style="color: #447ec9" href="http://www.speakit.cn/file/02%20DISCRIMINATIVE%20VIDEO%20REPRESENTATION%20WITH%20TEMPORAL%20ORDER%20FOR%20MICRO-EXPRESSION%20RECOGNITION%2008682295.pdf">paper</a>
      </p>	 
	  
      <p>
      <li><strong>Region based Robust Facial Expression Analysis</strong>
      <br>
      <strong>Zheng Lian</strong>, Ya Li, Jianhua Tao, Jian Huang, Mingyue Niu
      <br>
      <em>Proceedings of the first Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) </em> 2018 | <a style="color: #447ec9" href="http://www.speakit.cn/file/2018_Emotional%20computing_ACII2018_EI-Zheng%20Lian.pdf">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Speech Emotion Recognition via Contrastive Loss under Siamese Networks</strong>
      <br>
      <strong>Zheng Lian</strong>, Ya Li, Jianhua Tao, Jian Huang
      <br>
      <em>Proceedings of the Joint Workshop of the 4th Workshop on Affective Social Multimedia Computing and first Multi-Modal Affective Computing of Large-Scale Multimedia Data (ASMMC-MMAC) </em> 2018 | <a style="color: #447ec9" href="http://www.speakit.cn/file/2018_Emotional%20computing_ACMMM%202018-MMAC'18_EI-Zheng%20Lian.pdf">paper</a>
      </p>	
	  
	  <p>
      <li><strong>End-to-End Continuous Emotion Recognition from Video Using 3D ConvLSTM Networks</strong>
      <br>
	  Jian Huang, Ya Li, Jianhua Tao, <strong>Zheng Lian</strong>, Jiangyan Yi
      <br>
      <em>Proceedings of the 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) </em> 2018 | <a style="color: #447ec9" href="http://www.speakit.cn/file/2018_Emotional%20computing_ICASSP_EI-Jian%20Huang.pdf">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Speech Emotion Recognition from Variable-Length Inputs with Triplet Loss Function</strong>
      <br>
	  Jian Huang, Ya Li, Jianhua Tao, <strong>Zheng Lian</strong>
      <br>
      <em>Proceedings of the 19th Annual Conference of the International Speech Communication Association (Interspeech) </em> 2018 | <a style="color: #447ec9" href="http://www.speakit.cn/file/2018_Emotional%20computing_Interspeech%202018_EI-Jian%20Huang.pdf">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Multimodal continuous emotion recognition with data augmentation using recurrent neural networks</strong>
      <br>
	  Jian Huang, Ya Li, Jianhua Tao, <strong>Zheng Lian</strong>, Mingyue Niu, Minghao Yang
      <br>
      <em>Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop </em> 2018 | <a style="color: #447ec9" href="https://dl.acm.org/doi/abs/10.1145/3266302.3266304">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Deep learning for continuous multiple time series annotations</strong>
      <br>
	  Jian Huang, Ya Li, Jianhua Tao, <strong>Zheng Lian</strong>, Mingyue Niu, Minghao Yang
      <br>
      <em>Proceedings of the 2018 on Audio/Visual Emotion Challenge and Workshop </em> 2018 | <a style="color: #447ec9" href="https://dl.acm.org/doi/abs/10.1145/3266302.3266305">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Speech emotion recognition using semi-supervised learning with ladder networks</strong>
      <br>
	  Jian Huang, Ya Li, Jianhua Tao, <strong>Zheng Lian</strong>, Mingyue Niu, Jiangyan Yi
      <br>
      <em>Proceedings of the first Asian Conference on Affective Computing and Intelligent Interaction (ACII Asia) </em> 2018 | <a style="color: #447ec9" href="http://159.226.21.132/file/2018_Emotional%20computing_ACII2018_EI-Jian%20Huang.pdf">paper</a>
      </p>	
	  
	  <p>
      <li><strong>Continuous Multimodal Emotion Prediction Based on Long Short Term Memory Recurrent Neural Network</strong>
      <br>
	  Jian Huang, Ya Li, Jianhua Tao, <strong>Zheng Lian</strong>, Zhengqi Wen, Minghao Yang, Jiangyan Yi
      <br>
      <em>Proceedings of the 7th Annual Workshop on Audio/Visual Emotion Challenge </em> 2017 | <a style="color: #447ec9" href="http://www.speakit.cn/file/2017_SER_ACM%20AVEC%20huangjian.pdf">paper</a>
      </p>	
	  
      </ol>
	  

    </p>
  </div>


	
  <!-- The Awards Section -->
  <div class="w3-container w3-padding-32" id="award">
    <h2>Awards</h2>
	<p><li> 2023, The First Prize of MEGC@ACM MM Challenge</p>
	<p><li> 2023, Outstanding Reviewer of ICASSP</p>
	<p><li> 2022, The First Prize of MuSe@ACM MM Challenge</p>
	<p><li> 2021, The First prize for Technological Invention of Chinese Institute of Electronics</p>
	<!-- 中国电子学会技术发明一等奖 -->
	<p><li> 2021, The Best paper for National Conference on Man-Machine Speech Communication (NCMMSC)</p>
	<!-- 第十六届全国人机语音通讯学术会议最佳论文奖 -->
	<p><li> 2021, The First Prize of MuSe@ACM MM Challenge</p>
	<p><li> 2021, Climbing first-class scholarship of Institute of Automation, Chinese Academy of Sciences</p>
	<!-- 中国科学院自动化研究所攀登一等奖学金 -->
	<p><li> 2020, Climbing second-class scholarship of Institute of Automation, Chinese Academy of Sciences</p>
	<p><li> 2020, The First Prize of MuSe@ACM MM Challenge</p>
	<p><li> 2020, Interspeech Travel Grand</p>
	<p><li> 2020, The Second Prize of Voice Conversion Challenge</p>
    <p><li> 2019, The Second Prize of AVEC@ACM MM Challenge</p>
    <p><li> 2018, The Second Prize of AVEC@ACM MM Challenge</p>
	<!--<p><li> 2017, "Three Good Students" of University of Chinese Academy of Sciences</p>-->
  </div>  
  
  
  <!-- The Professionals Section -->
  <div class="w3-container w3-padding-32" id="professionals">
    <h2>Academic Services</h2>
	<p><li> Organize MER@ACM MM Grand Challenge and MRAC@ACM MM Workshop, 2023 </p>
	<p><li> Committee Member, Chinese Society of Image and Graphics, Emotional Computing and Understanding Committee（2021-） </p>
	<!-- 中国图象图形学学会情感计算与理解专委会委员 -->
	<p><li> Committee Member, Chinese Information Processing Society of China, Emotional Computing Committee（2021-） </p>
	<!-- 中国中文信息学会情感计算专委会委员 -->
	<p><li> Committee Member, Chinese Association for Artificial Intelligence, Emotional Intelligence Committee（2021-） </p>
	<!-- 中国人工智能学会情感智能专委会委员 -->
	<p><li> Reviewers for TNNLS, TASLP, TALLIP, TCSVT, IEEE Signal Processing Magazine, Information Fusion, Pattern Recognition, ACM Multimedia, ICASSP, Interspeech, ISCSLP, etc </p>
  </div> 
  

  <!-- The Projects Section -->
  <div class="w3-container w3-padding-32"" id="projects">
    <h2> Projects </h2>
	
    <ol>
	
      <p><li><strong>National Natural Science Foundation of China, Youth Science Fund Project, Research on Emotion Recognition Method for Interactive Scenes (Grant No. 62201572), 2023/1/1~2025/12/31, 300,000¥，Host</strong></p>
	  <!-- <p><li><strong>国家自然科学基金，青年科学基金项目，面向交互场景的情感识别方法研究，62201572，2023/1/1~2025/12/31，30万元，在研，主持</strong></p>-->
	  
    </p>
  </div>
  
  
  <!-- The Patents Section -->
  <div class="w3-container w3-padding-32"" id="patents">
    <h2> Patents </h2>
	
    <ol>
      
	  <p>
      <li><strong>Dialogue emotion correction method based on graph neural network (US Patent No. 17472511), 2022/08/25 </strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/4a/c7/22/47743d7416f139/US20220270636A1.pdf">Link</a>
      <br>
	  Jianhua Tao, <strong>Zheng Lian</strong>, Bin Liu, Xuefei Liu
      <br>
      </p>
	  
	  
	  <p>
      <li><strong>Automatic lie detection method and apparatus for interactive scenarios, device and medium (US Patent No. 11238289), 2022/02/01 </strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/5f/ef/96/54eda53532f316/US11238289.pdf">Link</a>
      <br>
	  Jianhua Tao, <strong>Zheng Lian</strong>, Bin Liu, Licai Sun
      <br>
      </p>
	  
	  
	  <p>
      <li><strong>Multimodal dimensional emotion recognition method (US Patent No. 11281945), 2022/03/22 </strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/28/6b/b6/d5d3ebfbcbeea4/US11281945.pdf">Link</a>
      <br>
	  Jianhua Tao, Licai Sun, Bin Liu, <strong>Zheng Lian</strong>
      <br>
      </p>
	  
	
	  <p>
      <li><strong>Multi-modal lie detection method and apparatus, and device (US Patent No. 11244119), 2022/02/08 </strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/c2/a6/4f/26204db8af95c5/US11244119.pdf">Link</a>
      <br>
	  Jianhua Tao, Licai Sun, Bin Liu, <strong>Zheng Lian</strong>
      <br>
      </p>
	  
	 
	
	  <p>
      <li><strong>Expression recognition method under natural scene (US Patent No. 11216652), 2022/01/04</strong>  <a style="color: #447ec9" href="https://patentimages.storage.googleapis.com/22/2c/bc/043136d205e9c5/US11216652.pdf">Link</a>
      <br>
	  Jianhua Tao, Mingyuan Xiao, Bin Liu, <strong>Zheng Lian</strong>
      <br>
      </p>
	  
	  <!--
	  <p>
      <li><strong>基于自回归网络的非平行语料语音转换方法及系统（已授权 CN112331183B 2022-03-08）</strong>
      <br>
	  <strong>连政</strong>、温正棋
      <br>
      </p>
	  -->
	  
	  <p>
      <li><strong>Non-parallel corpus speech conversion method and system based on autoregressive network (China Patent No. CN112331183B), 2022/03/08</strong>
      <br>
	  <strong>Zheng Lian</strong>, Zhengqi Wen
      <br>
      </p>
	  
	  <!--
	  <p>
      <li><strong>基于成对鉴别任务的语音情感识别方法与系统（已授权 CN108364662B 2021-01-05）</strong>
      <br>
	  陶建华、<strong>连政</strong>、李雅
      <br>
      </p>
	  -->
	  
	  <p>
      <li><strong>Speech emotion recognition method and system based on pairwise discrimination task (China Patent No. CN108364662B), 2021/01/05 </strong>
      <br>
	  Jianhua Tao, <strong>Zheng Lian</strong>, Ya Li
      <br>
      </p>
	  
	  <!--
	  <p>
      <li><strong>面向交互场景的自动谎言检测方法、装置、设备及介质（已授权 CN112329748B 2021-04-30）</strong>
      <br>
	  陶建华、<strong>连政</strong>、刘斌、孙立才
      <br>
      </p>
	  -->
	  
	  <!--
	  <p>
      <li><strong>基于图神经网络的对话情感纠错模型（已授权 CN112579745B 2021-06-08）</strong>
      <br>
	  陶建华、<strong>连政</strong>、刘斌、柳雪飞
      <br>
      </p>
	  -->
	  
	  <!--
	  <p>
      <li><strong>海量音视频情感识别系统（已授权 CN112633263B 2021-06-08）</strong>
      <br>
	  陶建华、<strong>连政</strong>、刘斌、孙立才
      <br>
      </p>
	  -->
	  
	  <p>
      <li><strong>Massive audio and video emotion recognition system (China Patent No. CN112633263B), 2021/06/08</strong>
      <br>
	  Jianhua Tao, <strong>Zheng Lian</strong>, Bin Liu, Licai Sun
      <br>
      </p>
	  
	  <!--
	  <p>
      <li><strong>基于音视频的鲁棒情感建模系统（已授权 CN113255800B 2021-10-15）</strong>
      <br>
	  陶建华、<strong>连政</strong>、刘斌、孙立才
      <br>
      </p>
	  -->
	  <p>
      <li><strong>Robust emotion modeling system based on audio and video (China Patent No. CN113255800B), 2021/10/15</strong>
      <br>
	  Jianhua Tao, <strong>Zheng Lian</strong>, Bin Liu, Licai Sun
      <br>
      </p>
	  
	  
	  <!--
	  <p>
      <li><strong>多模态谎言检测方法、装置、设备（已授权 CN112329746B 2021-04-16）</strong>
      <br>
	  陶建华、孙立才、刘斌、<strong>连政</strong>
      <br>
      </p>
	  -->
	  
	  <!--
	  <p>
      <li><strong>多模态维度情感识别方法（已授权 CN112560830B 2021-05-25）</strong>
      <br>
	  陶建华、孙立才、刘斌、<strong>连政</strong>
      <br>
      </p>
	  -->
	  
	  <!--
	  <p>
      <li><strong>自然场景下的表情识别方法和装置（已授权 CN112580617B 2021-06-18）</strong>
      <br>
	  陶建华、肖明远、刘斌、<strong>连政</strong>
      <br>
      </p>
	  -->
	  
	  <!--
	  <p>
      <li><strong>基于域对抗训练的自动谎言检测方法及系统（已申请 CN112329438A 2021-02-05）</strong>
      <br>
	  <strong>连政</strong>、刘斌、温正棋
      <br>
      </p>
	  -->
	  <p>
      <li><strong>Automatic lie detection method and system based on domain adversarial training (China Patent No. CN112329438A), 2021-02-05</strong>
      <br>
	  <strong>Zheng Lian</strong>, Bin Liu, Zhengqi Wen
      <br>
      </p>
	  
    </p>
  </div>
  
  
  
  <div class="w3-light-grey w3-center w3-padding-24">
  <script type="text/javascript">
  var sc_project=12347113; 
  var sc_invisible=0; 
  var sc_security="21aca5d1"; 
  var sc_https=1; 
  var scJsHost = "https://";
  document.write("<sc"+"ript type='text/javascript' src='" + scJsHost+
  "statcounter.com/counter/counter.js'></"+"script>");
  <noscript>
    <div class="statcounter"><a title="Web Analytics Made Easy -
  StatCounter" href="https://statcounter.com/" target="_blank"><img
  class="statcounter" src="https://c.statcounter.com/12347113/0/21aca5d1/0/"
  alt="Web Analytics Made Easy - StatCounter"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->

  </div>

  <!-- End page content -->
</div>

<script>
// Accordion 
function myAccFunc() {
  var x = document.getElementById("demoAcc");
  if (x.className.indexOf("w3-show") == -1) {
    x.className += " w3-show";
  } else {
    x.className = x.className.replace(" w3-show", "");
  }
}

// Click on the "Jeans" link on page load to open the accordion for demo purposes
document.getElementById("myBtn").click();


// Open and close sidebar
function w3_open() {
  document.getElementById("mySidebar").style.display = "block";
  document.getElementById("myOverlay").style.display = "block";
}
 
function w3_close() {
  document.getElementById("mySidebar").style.display = "none";
  document.getElementById("myOverlay").style.display = "none";
}
</script>

</body>
</html>
